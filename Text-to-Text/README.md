## Text-to-Text (LLMs)

### Python sample codes
* `bailong.py`
* `breeze.py`
* `gemma-1.1.py`
* `llama-2.py`
* `llama-3.py`
* `mamba.py` # for -130M, -370M, -790M, -1B, -3B, -3B-slimpj
* `mistral.py`
* `phi-2.py`
* `qwen1.5.py`
* `starling-lm.py`
* `taide-lx.py`
* `taiwan-llm.py`
* `vicuna.py`

---
### local-LLM Server & Client
* `python llm_server.py` (running on your PC's GPU)
* `python llm_client.py`
* `python post_text.py`

---
### Colab's LLM Server & Client
* [pyngrok-LLM server](https://github.com/rkuo2000/GenAI/blob/main/Text-to-Text/pyngrok_LLM_Server.ipynb) (on Colab T4)<br>
![](https://github.com/rkuo2000/GenAI/blob/main/assets/pyngrok_LLM_Server.png?raw=true)
* [post-text client](https://github.com/rkuo2000/GenAI/blob/main/Text-to-Text/post_text.py) (on your PC)<br>
![](https://github.com/rkuo2000/GenAI/blob/main/assets/pyngrok_post_text.png?raw=true)

